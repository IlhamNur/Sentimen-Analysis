{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load datasets from CSV files and handle NaN values\n",
    "def load_datasets(train_file, label_file, test_file):\n",
    "    train_data = pd.read_csv(train_file).fillna(0).values\n",
    "    train_labels = pd.read_csv(label_file).fillna(0).values.flatten()\n",
    "    test_data = pd.read_csv(test_file).fillna(0).values\n",
    "    return train_data, train_labels, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate cosine similarity between two datasets\n",
    "def calculate_cosine_similarity(train_vectors, test_vector):\n",
    "    similarities = cosine_similarity(train_vectors, test_vector.reshape(1, -1)).flatten()\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform KNN with improved steps and save results to CSV\n",
    "def knn_improved(train_data, train_labels, test_vector, k):\n",
    "    if len(train_data) == 0 or len(test_vector) == 0:\n",
    "        return None, None\n",
    "\n",
    "    # Step 1: Calculate similarity between test and train data\n",
    "    similarities = calculate_cosine_similarity(train_data, test_vector)\n",
    "    \n",
    "    # Save step 1 results\n",
    "    pd.DataFrame({\"similarities\": similarities}).to_csv(\"step1_similarities.csv\", index=False)\n",
    "\n",
    "    # Step 2: Sort similarities and get top k neighbors\n",
    "    sorted_indices = np.argsort(similarities)[::-1]  # Descending order\n",
    "    top_k_indices = sorted_indices[:k]\n",
    "    top_k_similarities = similarities[top_k_indices]\n",
    "    top_k_labels = train_labels[top_k_indices]\n",
    "    \n",
    "    # Save step 2 results\n",
    "    pd.DataFrame({\"top_k_similarities\": top_k_similarities, \"top_k_labels\": top_k_labels}).to_csv(\"step2_top_k.csv\", index=False)\n",
    "\n",
    "    # Step 3: Calculate n (new k-values) per category\n",
    "    unique_labels = np.unique(train_labels)\n",
    "    n_values = {label: 0 for label in unique_labels}\n",
    "    for label in top_k_labels:\n",
    "        n_values[label] += 1\n",
    "    \n",
    "    # Save step 3 results\n",
    "    pd.DataFrame.from_dict(n_values, orient='index', columns=['n_values']).to_csv(\"step3_n_values.csv\")\n",
    "\n",
    "    # Step 4: Calculate similarity comparison per category\n",
    "    similarity_sums = {label: 0 for label in unique_labels}\n",
    "    for idx, label in enumerate(top_k_labels):\n",
    "        similarity_sums[label] += top_k_similarities[idx]\n",
    "    \n",
    "    # Save step 4 results\n",
    "    pd.DataFrame.from_dict(similarity_sums, orient='index', columns=['similarity_sums']).to_csv(\"step4_similarity_sums.csv\")\n",
    "\n",
    "    # Step 5: Calculate the maximum comparison ratio\n",
    "    max_comparison_ratio = {}\n",
    "    total_similarity = sum(top_k_similarities)\n",
    "    if total_similarity > 0:\n",
    "        for label in unique_labels:\n",
    "            max_comparison_ratio[label] = similarity_sums[label] / total_similarity\n",
    "    else:\n",
    "        for label in unique_labels:\n",
    "            max_comparison_ratio[label] = 0\n",
    "    \n",
    "    # Save step 5 results\n",
    "    pd.DataFrame.from_dict(max_comparison_ratio, orient='index', columns=['max_comparison_ratio']).to_csv(\"step5_max_comparison_ratio.csv\")\n",
    "\n",
    "    # Return the category with the highest ratio\n",
    "    if max_comparison_ratio:\n",
    "        predicted_label = max(max_comparison_ratio, key=max_comparison_ratio.get)\n",
    "        return predicted_label, max_comparison_ratio\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels: [np.int64(1), np.int64(1), np.int64(-1), np.int64(-1), np.int64(1), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(1), np.int64(-1), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(-1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(-1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(-1), np.int64(-1), np.int64(0), np.int64(-1), np.int64(-1), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(-1), np.int64(-1), np.int64(-1), np.int64(1), np.int64(-1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(-1), np.int64(-1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(-1), np.int64(-1), np.int64(-1), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(0), np.int64(-1), np.int64(1), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(-1), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(-1), np.int64(1), np.int64(1), np.int64(-1), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(-1), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(-1), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(0), np.int64(-1), np.int64(-1), np.int64(1), np.int64(1), np.int64(-1), np.int64(0), np.int64(-1), np.int64(0), np.int64(1), np.int64(-1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(-1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(-1), np.int64(-1), np.int64(1), np.int64(-1), np.int64(1), np.int64(1), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(-1), np.int64(1), np.int64(-1), np.int64(-1), np.int64(-1), np.int64(0), np.int64(-1), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(-1), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(-1), np.int64(0), np.int64(1), np.int64(1), np.int64(-1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(-1), np.int64(1), np.int64(1), np.int64(-1), np.int64(1), np.int64(1), np.int64(-1), np.int64(1), np.int64(-1), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(-1), np.int64(1), np.int64(1), np.int64(-1), np.int64(-1), np.int64(0), np.int64(-1), np.int64(1), np.int64(1), np.int64(1), np.int64(0), np.int64(0), np.int64(1), np.int64(-1), np.int64(1), np.int64(1), np.int64(-1), np.int64(-1), np.int64(1), np.int64(1), np.int64(-1), np.int64(1), np.int64(0), np.int64(-1), np.int64(-1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(-1), np.int64(-1), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(-1), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(-1), np.int64(1), np.int64(1), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(-1), np.int64(0), np.int64(-1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(-1), np.int64(0), np.int64(0), np.int64(1)]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "train_file = \"train_tfidf.csv\"\n",
    "label_file = \"train_label.csv\"\n",
    "test_file = \"test_tfidf.csv\"\n",
    "k = 12\n",
    "\n",
    "train_vectors, train_labels, test_vectors = load_datasets(train_file, label_file, test_file)\n",
    "predicted_labels = []\n",
    "\n",
    "for test_vector in test_vectors:\n",
    "    result = knn_improved(train_vectors, train_labels, test_vector, k)\n",
    "    if result[0] is not None:\n",
    "        predicted_labels.append(result[0])\n",
    "    else:\n",
    "        predicted_labels.append(\"Error\")\n",
    "\n",
    "print(\"Predicted Labels:\", predicted_labels)\n",
    "pd.DataFrame({\"predicted_labels\": predicted_labels}).to_csv(\"predicted_labels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Function to evaluate model performance\n",
    "def evaluate_model(predicted_labels, true_labels):\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels, average='weighted', zero_division=0)\n",
    "    recall = recall_score(true_labels, predicted_labels, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='weighted', zero_division=0)\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load true labels for evaluation\n",
    "true_labels = pd.read_csv(\"test_label.csv\").fillna(0).values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model with multiple values of k\n",
    "k_values = [1, 5, 10, 15, 20, 25, 30, 35, 40, 50, 60]\n",
    "results = []\n",
    "all_predictions = {\"true_labels\": true_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in k_values:\n",
    "    predicted_labels = []\n",
    "    n_values_list = []  # To store n values for each test vector\n",
    "\n",
    "    for test_vector in test_vectors:\n",
    "        result = knn_improved(train_vectors, train_labels, test_vector, k)\n",
    "\n",
    "        # Extract predicted label and n values from the result\n",
    "        if result[0] is not None:\n",
    "            predicted_labels.append(result[0])\n",
    "\n",
    "            # Calculate n values per category (step 3 in knn_improved)\n",
    "            similarities = calculate_cosine_similarity(train_vectors, test_vector)\n",
    "            sorted_indices = np.argsort(similarities)[::-1][:k]\n",
    "            top_k_labels = train_labels[sorted_indices]\n",
    "\n",
    "            # Calculate n values for each label\n",
    "            unique_labels, counts = np.unique(top_k_labels, return_counts=True)\n",
    "            n_values = dict(zip(unique_labels, counts))\n",
    "            total_n = sum(n_values.values())\n",
    "            n_values_list.append(total_n)\n",
    "        else:\n",
    "            predicted_labels.append(-1)  # Error or undefined prediction\n",
    "            n_values_list.append(0)  # Assign 0 if undefined\n",
    "\n",
    "    # Add predictions for the current k to the dictionary\n",
    "    all_predictions[f\"predictions_k_{k}\"] = predicted_labels\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy, precision, recall, f1 = evaluate_model(predicted_labels, true_labels)\n",
    "\n",
    "    # Calculate average n value\n",
    "    avg_n_value = np.mean(n_values_list)\n",
    "\n",
    "    # Store results\n",
    "    results.append({\n",
    "        \"k\": k,\n",
    "        \"n (nilai k baru)\": avg_n_value,\n",
    "        \"Akurasi\": accuracy,\n",
    "        \"Presisi\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"Skor F1\": f1\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     k  n (nilai k baru)   Akurasi   Presisi    Recall   Skor F1\n",
      "0    1               1.0  0.837748  0.841654  0.837748  0.838765\n",
      "1    5               5.0  0.897351  0.902071  0.897351  0.897716\n",
      "2   10              10.0  0.947020  0.947905  0.947020  0.947182\n",
      "3   15              15.0  0.960265  0.962621  0.960265  0.960165\n",
      "4   20              20.0  0.917219  0.919610  0.917219  0.916146\n",
      "5   25              25.0  0.903974  0.907117  0.903974  0.902417\n",
      "6   30              30.0  0.890728  0.894940  0.890728  0.888261\n",
      "7   35              35.0  0.877483  0.881325  0.877483  0.874576\n",
      "8   40              40.0  0.857616  0.860629  0.857616  0.853693\n",
      "9   50              50.0  0.844371  0.848458  0.844371  0.839412\n",
      "10  60              60.0  0.827815  0.833695  0.827815  0.821567\n"
     ]
    }
   ],
   "source": [
    "# Save results to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"model_evaluation_results_with_n.csv\", index=False)\n",
    "\n",
    "# Print results for verification\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all predictions to a separate CSV file\n",
    "predictions_df = pd.DataFrame(all_predictions)\n",
    "predictions_df.to_csv(\"all_predictions_by_k.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation Results:\n",
      "     k  n (nilai k baru)   Akurasi   Presisi    Recall   Skor F1\n",
      "0    1               1.0  0.837748  0.841654  0.837748  0.838765\n",
      "1    5               5.0  0.897351  0.902071  0.897351  0.897716\n",
      "2   10              10.0  0.947020  0.947905  0.947020  0.947182\n",
      "3   15              15.0  0.960265  0.962621  0.960265  0.960165\n",
      "4   20              20.0  0.917219  0.919610  0.917219  0.916146\n",
      "5   25              25.0  0.903974  0.907117  0.903974  0.902417\n",
      "6   30              30.0  0.890728  0.894940  0.890728  0.888261\n",
      "7   35              35.0  0.877483  0.881325  0.877483  0.874576\n",
      "8   40              40.0  0.857616  0.860629  0.857616  0.853693\n",
      "9   50              50.0  0.844371  0.848458  0.844371  0.839412\n",
      "10  60              60.0  0.827815  0.833695  0.827815  0.821567\n",
      "\n",
      "All Predictions by k:\n",
      "     true_labels  predictions_k_1  predictions_k_5  predictions_k_10  \\\n",
      "0              1                1                1                 1   \n",
      "1              1                1                1                 1   \n",
      "2             -1               -1               -1                -1   \n",
      "3             -1               -1               -1                -1   \n",
      "4              1                1                1                 1   \n",
      "..           ...              ...              ...               ...   \n",
      "297            1                1                1                 1   \n",
      "298           -1               -1               -1                -1   \n",
      "299            0                0                0                 0   \n",
      "300            0                0                0                 0   \n",
      "301            1                1                1                 1   \n",
      "\n",
      "     predictions_k_15  predictions_k_20  predictions_k_25  predictions_k_30  \\\n",
      "0                   1                 1                 1                 1   \n",
      "1                   1                 1                 1                 1   \n",
      "2                  -1                -1                -1                 1   \n",
      "3                  -1                -1                -1                -1   \n",
      "4                   1                 1                 1                 1   \n",
      "..                ...               ...               ...               ...   \n",
      "297                 1                 1                 1                 1   \n",
      "298                -1                -1                -1                -1   \n",
      "299                 0                 0                 0                 0   \n",
      "300                 0                 0                 0                 1   \n",
      "301                 1                 1                 1                 1   \n",
      "\n",
      "     predictions_k_35  predictions_k_40  predictions_k_50  predictions_k_60  \n",
      "0                   1                 1                 1                 1  \n",
      "1                   1                 1                 1                 1  \n",
      "2                   1                 1                 1                 1  \n",
      "3                  -1                -1                -1                -1  \n",
      "4                   1                 1                 1                 1  \n",
      "..                ...               ...               ...               ...  \n",
      "297                 1                 1                 1                 1  \n",
      "298                -1                -1                -1                -1  \n",
      "299                 0                 0                 0                 0  \n",
      "300                 1                 1                 1                 1  \n",
      "301                 1                 1                 1                 1  \n",
      "\n",
      "[302 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "# Print results for verification\n",
    "print(\"Model Evaluation Results:\")\n",
    "print(results_df)\n",
    "print(\"\\nAll Predictions by k:\")\n",
    "print(predictions_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
