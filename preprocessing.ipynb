{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from deep_translator import GoogleTranslator\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 18:42:12,821 - INFO - Dataset loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv('dataset.csv')\n",
    "    logging.info(\"Dataset loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    logging.error(f\"Error loading dataset: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 18:42:12,913 - INFO - Synonym dictionary loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load synonym dictionary\n",
    "try:\n",
    "    synonym_df = pd.read_csv('colloquial-indonesian-lexicon.csv')\n",
    "    synonym_dict = dict(zip(synonym_df['slang'], synonym_df['formal']))\n",
    "    logging.info(\"Synonym dictionary loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    logging.error(f\"Error loading synonym dictionary: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 18:42:12,934 - INFO - Stopwords loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load stopwords from external files\n",
    "try:\n",
    "    with open('id.stopwords.02.01.2016.txt', 'r', encoding='utf-8') as file:\n",
    "        stopwords = set(file.read().splitlines())\n",
    "    logging.info(\"Stopwords loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    logging.error(f\"Error loading stopwords: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi translator\n",
    "translator = GoogleTranslator(source='auto', target='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing functions\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text)\n",
    "    text = re.sub(r'[\\U00010000-\\U0010FFFF]', '', text, flags=re.UNICODE)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cf_text(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = re.split(r'\\W+', text)\n",
    "    return [token for token in tokens if token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(tokens):\n",
    "    return [\n",
    "        stemmer.stem(token)\n",
    "        for token in tokens\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_to_bahasa(tokens):\n",
    "    try:\n",
    "        translations = translator.translate_batch(tokens, batch_size=10)\n",
    "        return translations\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Translation failed: {e}\")\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_synonyms(words, synonym_dict):\n",
    "    return [synonym_dict.get(word, word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_columns = df.select_dtypes(include=['object']).columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 18:42:13,189 - INFO - Starting text cleaning...\n",
      "2025-01-13 18:42:13,224 - INFO - Text cleaning completed.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Clean text\n",
    "logging.info(\"Starting text cleaning...\")\n",
    "df_cleaned = df.copy()\n",
    "for col in text_columns:\n",
    "    df_cleaned[col] = df_cleaned[col].apply(lambda x: clean_text(x) if isinstance(x, str) else x)\n",
    "df_cleaned.to_csv('cleaned_dataset.csv', index=False)\n",
    "logging.info(\"Text cleaning completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 18:42:13,242 - INFO - Starting case folding...\n",
      "2025-01-13 18:42:13,253 - INFO - Case folding completed.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Case folding\n",
    "logging.info(\"Starting case folding...\")\n",
    "df_cf = df_cleaned.copy()\n",
    "for col in text_columns:\n",
    "    df_cf[col] = df_cf[col].apply(lambda x: cf_text(x) if isinstance(x, str) else x)\n",
    "df_cf.to_csv('cf_dataset.csv', index=False)\n",
    "logging.info(\"Case folding completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 18:42:13,291 - INFO - Starting tokenization...\n",
      "2025-01-13 18:42:13,317 - INFO - Tokenization completed.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Tokenization\n",
    "logging.info(\"Starting tokenization...\")\n",
    "df_tokenized = df_cf.copy()\n",
    "for col in text_columns:\n",
    "    df_tokenized[col] = df_tokenized[col].apply(lambda x: tokenize_text(x) if isinstance(x, str) else x)\n",
    "df_tokenized.to_csv('tokenized_dataset.csv', index=False)\n",
    "logging.info(\"Tokenization completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 18:42:13,332 - INFO - Removing stopwords...\n",
      "2025-01-13 18:42:13,351 - INFO - Stopwords removed.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Remove stopwords\n",
    "logging.info(\"Removing stopwords...\")\n",
    "df_sr = df_tokenized.copy()\n",
    "for col in text_columns:\n",
    "    df_sr[col] = df_sr[col].apply(lambda x: remove_stopwords(translate_to_bahasa(x)) if isinstance(x, list) else x)\n",
    "df_sr.to_csv('stopword_removed_dataset.csv', index=False)\n",
    "logging.info(\"Stopwords removed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 18:42:13,399 - INFO - Starting stemming...\n",
      "2025-01-13 18:43:13,761 - INFO - Stemming completed.\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Stemming\n",
    "logging.info(\"Starting stemming...\")\n",
    "df_stemmed = df_sr.copy()\n",
    "for col in text_columns:\n",
    "    df_stemmed[col] = df_stemmed[col].apply(lambda x: stemming(x) if isinstance(x, list) else x)\n",
    "df_stemmed.to_csv('stemmed_dataset.csv', index=False)\n",
    "logging.info(\"Stemming completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 18:43:13,797 - INFO - Starting synonym replacement...\n",
      "2025-01-13 18:43:13,844 - INFO - Synonym replacement completed.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Synonym replacement\n",
    "logging.info(\"Starting synonym replacement...\")\n",
    "df_synonym_replaced = df_stemmed.copy()\n",
    "for col in text_columns:\n",
    "    df_synonym_replaced[col] = df_synonym_replaced[col].apply(\n",
    "        lambda x: replace_with_synonyms(x, synonym_dict) if isinstance(x, list) else x\n",
    "    )\n",
    "df_synonym_replaced.to_csv('synonym_replaced_dataset.csv', index=False)\n",
    "logging.info(\"Synonym replacement completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Tweet\n",
      "0        [kampus, keren, banget, fasilitas, lengkap]\n",
      "1       [duh, tugas, kuliah, numpuk, banget, pusing]\n",
      "2                       [tugas, kerja, minggu, seru]\n",
      "3  [kanan, dosen, asih, nilai, sulit, mahasiswa, ...\n",
      "4  [tau, anjyr, muak, gue, ajar, bisnis, internas...\n"
     ]
    }
   ],
   "source": [
    "# Display the final dataset\n",
    "print(df_synonym_replaced.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
